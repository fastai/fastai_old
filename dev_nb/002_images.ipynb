{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nb_001b import *\n",
    "import sys, PIL, matplotlib.pyplot as plt, itertools, math, random, collections, torch\n",
    "import scipy.stats, scipy.special\n",
    "\n",
    "from enum import Enum, IntEnum\n",
    "from torch import tensor, FloatTensor, LongTensor, ByteTensor, DoubleTensor, HalfTensor, ShortTensor\n",
    "from operator import itemgetter, attrgetter\n",
    "from numpy import cos, sin, tan, tanh, log, exp\n",
    "\n",
    "from functools import reduce\n",
    "from collections import defaultdict, abc, namedtuple, Iterable\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR subset data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to view our data to check if everything is how we expect it to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('data')\n",
    "PATH = DATA_PATH/'cifar10_dog_air'\n",
    "TRAIN_PATH = PATH/'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_fn = list((TRAIN_PATH/'dog').iterdir())[0]\n",
    "dog_image = Image.open(dog_fn)\n",
    "dog_image.resize((256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_fn = list((TRAIN_PATH/'airplane').iterdir())[0]\n",
    "air_image = Image.open(air_fn)\n",
    "air_image.resize((256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Dataset/Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a Dataset class for our image files. A Dataset class needs to have two functions: length and get-item. Our FilesDataset additionally gets the image files from their respective directories and transforms them to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_classes(folder):\n",
    "    classes = [d for d in folder.iterdir()\n",
    "               if d.is_dir() and not d.name.startswith('.')]\n",
    "    assert(len(classes)>0)\n",
    "    return sorted(classes, key=lambda d: d.name)\n",
    "\n",
    "def get_image_files(c):\n",
    "    return [o for o in list(c.iterdir())\n",
    "            if not o.name.startswith('.') and not o.is_dir()]\n",
    "\n",
    "def pil2tensor(image):\n",
    "    arr = torch.ByteTensor(torch.ByteStorage.from_buffer(image.tobytes()))\n",
    "    arr = arr.view(image.size[1], image.size[0], -1)\n",
    "    arr = arr.permute(2,0,1)\n",
    "    return arr.float().div_(255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FilesDataset(Dataset):\n",
    "    def __init__(self, folder, classes=None):\n",
    "        self.fns, self.y = [], []\n",
    "        if classes is None: classes = [cls.name for cls in find_classes(folder)]\n",
    "        self.classes = classes\n",
    "        for i, cls in enumerate(classes):\n",
    "            fnames = get_image_files(folder/cls)\n",
    "            self.fns += fnames\n",
    "            self.y += [i] * len(fnames)\n",
    "        \n",
    "    def __len__(self): return len(self.fns)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        x = PIL.Image.open(self.fns[i]).convert('RGB')\n",
    "        return pil2tensor(x),self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FilesDataset(PATH/'train')\n",
    "valid_ds = FilesDataset(PATH/'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds), len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def image2np(image): return image.cpu().permute(1,2,0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = train_ds[0]\n",
    "plt.imshow(image2np(x))\n",
    "print(train_ds.classes[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(train_ds, valid_ds, bs=bs)\n",
    "len(data.train_dl), len(data.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_image(img, ax=None, figsize=(3,3), hide_axis=True):\n",
    "    if ax is None: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(image2np(img))\n",
    "    if hide_axis: ax.axis('off')\n",
    "\n",
    "def show_image_batch(dl, classes, rows=None, figsize=(12,15)):\n",
    "    x,y = next(iter(dl))\n",
    "    if rows is None: rows = int(math.sqrt(len(x)))\n",
    "    show_images(x[:rows*rows],y[:rows*rows],rows, classes)\n",
    "\n",
    "def show_images(x,y,rows, classes, figsize=(9,9)):\n",
    "    fig, axs = plt.subplots(rows,rows,figsize=figsize)\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        show_image(x[i], ax)\n",
    "        ax.set_title(classes[y[i]])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_batch(data.train_dl, train_ds.classes, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going augment our data to increase our training set with artificial images. These new images are basically \"free\" data that we can use in our training to help our model generalize better (reduce overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by changing the **brightness** and **contrast** of our images.\n",
    "\n",
    "### Method\n",
    "\n",
    "**Brightness**\n",
    "\n",
    "Brightness refers to where does our image stand on the dark-light spectrum. Brightness is applied by adding a positive constant to each of the image's channels. This works because each of the channels in an image goes from 0 (darkest) to 255 (brightest) in a dark-light continum. (0, 0, 0) is black (total abscence of light) and (255, 255, 255) is white (pure light). You can check how this works by experimenting by yourself [here](https://www.w3schools.com/colors/colors_rgb.asp).\n",
    "\n",
    "_Parameters_\n",
    "\n",
    "1. **Change** How much brightness do we want to add to (or take from) the image.\n",
    "\n",
    "    $C \\in \\mathbb{R}$\n",
    "    \n",
    "**Contrast**\n",
    "\n",
    "Contrast refers to how sharp a distinction there is between birghter and darker sections of our image. To increase contrast we need darker pixels to be darker and lighter pixels to be lighter. In other words, we would like channels with a value smaller than 128 to decrease and channels with a value of greater than 128 to increase.\n",
    "\n",
    "_Parameters_\n",
    "\n",
    "1. **Scale** How much contrast do we want to add to (or remove from) the image.\n",
    "\n",
    "    $C \\in [0, inf]$\n",
    "    \n",
    "***On logit and sigmoid***\n",
    "\n",
    "Notice that for both transformations we first apply the logit to our tensor, then apply the transformation and finally take the sigmoid. This is important for two reasons. \n",
    "\n",
    "First, we don't want to overflow our tensor values. In other words, we need our final tensor values to be $T_{ij} \\in [0,1]$.  Imagine, for instance, a tensor value at 0.99. We want to increase its brightness, but we canâ€™t go over 1.0. By doing logit first, which first moves our space to $-inf$ to $+inf$, this works fine. The same applies to contrast if we have a scale $S > 1$ (might make some of our tensor values greater than one).\n",
    "\n",
    "Second, when we apply contrast, we need to affect the dispersion of values around the middle value. Say we want to increase contrast. Then we need the bright values ($>0.5$) to get brighter and dark values ($<0.5$) to get darker. We must first transform our tensor values so our values which were originally $<0.5$ are now negative and our values which were originally $>0.5$ are positive. This way, when we multiply by a constant, the dispersion around 0 will increase. The logit function does exactly this and allows us to increase or decrease dispersion around a mid value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def logit(x): return (x/(1-x)).log()\n",
    "def logit_(x): return (x.div_(1-x)).log_()\n",
    "\n",
    "def brightness(x, change): return x.add_(scipy.special.logit(change))\n",
    "def contrast(x, scale): return x.mul_(scale)\n",
    "\n",
    "def _apply_lighting(x, func): return func(logit_(x)).sigmoid()\n",
    "def apply_lighting(func):     return partial(_apply_lighting, func=func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_contrast(scale): return apply_lighting(partial(contrast, scale=scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lambda: train_ds[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(1,4, figsize=(12,3))\n",
    "\n",
    "show_image(x(), axes[0])\n",
    "show_image(apply_contrast(1.0)(x()), axes[1])\n",
    "show_image(apply_contrast(0.5)(x()), axes[2])\n",
    "show_image(apply_contrast(2.0)(x()), axes[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_brightness(change):\n",
    "    return apply_lighting(partial(brightness, change=change))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(1,4, figsize=(12,3))\n",
    "\n",
    "show_image(x(), axes[0])\n",
    "show_image(apply_brightness(0.5)(x()), axes[1])\n",
    "show_image(apply_brightness(0.8)(x()), axes[2])\n",
    "show_image(apply_brightness(0.2)(x()), axes[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def listify(p=None, q=None):\n",
    "    if p is None: p=[]\n",
    "    elif not isinstance(p, Iterable): p=[p]\n",
    "    n = q if type(q)==int else 1 if q is None else len(q)\n",
    "    if len(p)==1: p = p * n\n",
    "    return p\n",
    "\n",
    "def compose(funcs):\n",
    "    def _inner(x, *args, **kwargs):\n",
    "        for f in funcs: x = f(x, *args, **kwargs)\n",
    "        return x\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_brightness_contrast(scale_contrast, change_brightness):\n",
    "    return apply_lighting(compose([\n",
    "        partial(contrast, scale=scale_contrast),\n",
    "        partial(brightness, change=change_brightness)\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(1,4, figsize=(12,3))\n",
    "\n",
    "show_image(apply_brightness_contrast(0.75, 0.7)(x()), axes[0])\n",
    "show_image(apply_brightness_contrast(1.3,  0.3)(x()), axes[1])\n",
    "show_image(apply_brightness_contrast(1.3,  0.7)(x()), axes[2])\n",
    "show_image(apply_brightness_contrast(0.75, 0.3)(x()), axes[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random lighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will make our previous transforms random since we are interested in automatizing the pipeline. We will achieve this by making our parameters stochastic with a specific distribution. \n",
    "\n",
    "We will use a <a href=\"https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)\"> uniform</a> distribution for brightness change since its domain is the real numbers and the impact varies linearly with the scale. For contrast we use [log_uniform](https://www.vosesoftware.com/riskwiki/LogUniformdistribution.php) for two reasons. First, contrast scale has a domain of $[0, inf]$. Second, the impact of the scale in the transformation is non-linear (i.e. 0.5 is as extreme as 2.0, 0.2 is as extreme as 5). The log_uniform function is appropriate because it has the same domain and correctly represents the non-linearity of the transform, $P(0.5) = P(2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def uniform(low, high, size=None):\n",
    "    return random.uniform(low,high) if size is None else torch.FloatTensor(size).uniform_(low,high)\n",
    "\n",
    "def log_uniform(low, high, size=None):\n",
    "    res = uniform(log(low), log(high), size)\n",
    "    return exp(res) if size is None else res.exp_()\n",
    "\n",
    "def rand_bool(p, size=None): return uniform(0,1,size)<p\n",
    "\n",
    "TfmType = IntEnum('TfmType', 'Start Affine Coord Pixel Lighting')\n",
    "\n",
    "def brightness(x, change:uniform) -> TfmType.Lighting:\n",
    "    return x.add_(scipy.special.logit(change))\n",
    "\n",
    "def contrast(x, scale:log_uniform) -> TfmType.Lighting:\n",
    "    return x.mul_(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brightness.__annotations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.gmean([log_uniform(0.5,2.0) for _ in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import inspect\n",
    "\n",
    "def get_default_args(func):\n",
    "    return {k: v.default\n",
    "            for k, v in inspect.signature(func).parameters.items()\n",
    "            if v.default is not inspect.Parameter.empty}\n",
    "\n",
    "def resolve_args(func, **kwargs):\n",
    "    kwargs.pop('p', None)\n",
    "    def_args = get_default_args(func)\n",
    "    for k,v in func.__annotations__.items():\n",
    "        if k == 'return': continue\n",
    "        if not k in kwargs and k in def_args:\n",
    "            kwargs[k] = def_args[k]\n",
    "        else:\n",
    "            arg = listify(kwargs.get(k, 1))\n",
    "            kwargs[k] = v(*arg)\n",
    "    return kwargs\n",
    "\n",
    "def noop(x=None, *args, **kwargs): return x\n",
    "\n",
    "class Transform():\n",
    "    def __init__(self, func, **kwargs):\n",
    "        self.func,self.kw = func,kwargs\n",
    "        self.tfm_type = self.func.__annotations__['return']\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'{self.func.__name__}_tfm->{self.tfm_type.name}; {self.kw}'\n",
    "    \n",
    "    def __call__(self):\n",
    "        if 'p' in self.kw and not rand_bool(self.kw['p']): return noop\n",
    "        kwargs = resolve_args(self.func, **self.kw)\n",
    "        return partial(self.func, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolve_args(brightness, change=(0.25,0.75),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_tfm = partial(Transform, contrast)\n",
    "tfm = contrast_tfm(scale=(0.3,3))\n",
    "tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the same\n",
    "tfm = tfm()\n",
    "\n",
    "_,axes = plt.subplots(1,4, figsize=(12,3))\n",
    "for ax in axes: show_image(apply_lighting(tfm)(x()), ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = contrast_tfm(scale=(0.3,3), p=0.5)\n",
    "\n",
    "# different\n",
    "_,axes = plt.subplots(1,4, figsize=(12,3))\n",
    "for ax in axes: show_image(apply_lighting(tfm())(x()), ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decorator and composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in composing the transform functions so as to apply them all at once. We will try to feed a list of transforms to our pipeline for it to apply all of them.\n",
    "\n",
    "Applying a function to our transforms before calling them in Python is easiest if we use a decorator. You can find more about decorators [here](https://www.thecodeship.com/patterns/guide-to-python-function-decorators/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def reg_partial(cl, func):\n",
    "    setattr(sys.modules[func.__module__], f'{func.__name__}_tfm', partial(cl,func))\n",
    "    return func\n",
    "\n",
    "def reg_transform(func): return reg_partial(Transform, func)\n",
    "\n",
    "def resolve_tfms(tfms): return [f() for f in listify(tfms)]\n",
    "\n",
    "@reg_transform\n",
    "def brightness(x, change: uniform) -> TfmType.Lighting:  return x.add_(scipy.special.logit(change))\n",
    "\n",
    "@reg_transform\n",
    "def contrast(x, scale: log_uniform) -> TfmType.Lighting: return x.mul_(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tfms(tfms):\n",
    "    func = apply_lighting(compose(resolve_tfms(tfms)))\n",
    "    return lambda x: func(x.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_ds[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [contrast_tfm(scale=(0.3,3.0), p=0.5),\n",
    "        brightness_tfm(change=(0.35,0.65), p=0.5)]\n",
    "\n",
    "_,axes = plt.subplots(1,4, figsize=(12,3))\n",
    "for ax in axes: show_image(apply_tfms(tfms)(x), ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(2,4, figsize=(12,6))\n",
    "for i in range(4):\n",
    "    tfm = apply_tfms(tfms)\n",
    "    show_image(tfm(x), axes[0][i])\n",
    "    show_image(tfm(x), axes[1][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now add affine transforms that operate on the coordinates instead of pixels like the lighting transforms we just saw. An [affine transformation](https://en.wikipedia.org/wiki/Affine_transformation) is a function \"(...) between affine spaces which preserves points, straight lines and planes.\" It is basically a transformation $f:\\mathcal{X}\\mapsto\\mathcal{Y}$ of the form $ \\mathcal{x}\\mapsto \\mathcal{M}\\mathcal{x}+\\mathcal{b}$ where $\\mathcal{M}$ is a linear transformation on $\\mathcal{X}$ and $\\mathcal{b}$ is a vector in $\\mathcal{Y}$.\n",
    "\n",
    "\n",
    "### Affine Method\n",
    "\n",
    "Our implementation first creates a grid of coordinates for the original image. The grid is normalized to a [-1, 1] range with (-1, -1) representing the top left corner, (1, 1) the bottom right corner and (0, 0) the center. Next, we build an affine matrix representing our desired transform and we multiply it by our original grid coordinates. The result will be a set of x, y coordinates which references where in the input image will each of the pixels in the output image be mapped. It has a size of w \\* h \\* 2 since it needs two coordinates for each of the h * w pixels of the output image. \n",
    "\n",
    "This is clearest if we see it graphically. We will build an affine matrix of the following form:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & b & c \\\\\n",
    "c & d  & f\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "with which we will transform each pair of x, y coordinates in our original grid into our transformation grid:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d \\\\\n",
    "\\end{bmatrix} \n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "c \\\\\n",
    "f \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x^\\prime \\\\\n",
    "y^\\prime \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So after the transform we will get a new grid with which to map our input image into our output image. This will be our **map of where from exactly does our transformation source each pixel in the output image**:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1, -1 & x_2, -1 & ... & x_{n/2}, -1 & ... & 1, -1 \\\\\n",
    "-1, y_2 & x_2, y_2  & ... & x_{n/2}, y_2 & ... & 1, y_2 \\\\\n",
    "\\vdots   & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ \n",
    "-1, y_{n/2} & x_2, y_{n/2} & ... & 0, 0 & ... & 1, y_{n/2} \\\\\n",
    "\\vdots   & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ \n",
    "-1, 1 & x_2, 1 & ... & x_{n/2}, 1 & ... & 1, 1        \\\\\n",
    "\\end{bmatrix}\n",
    "\\longmapsto\n",
    "\\begin{bmatrix}\n",
    "x^\\prime_1, y^\\prime_1 & x^\\prime_2, y^\\prime_1 & ... & x^\\prime_{n/2}, y^\\prime_1 & ... & ^\\prime_n, y^\\prime_1 \\\\\n",
    "x^\\prime_1, y^\\prime_2 & x^\\prime_2, y^\\prime_2  & ... & x^\\prime_{n/2}, y^\\prime_2 & ... & x^\\prime_n, y^\\prime_2 \\\\\n",
    "\\vdots   & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ \n",
    "x^\\prime_1, y^\\prime_{n/2} & x^\\prime_2, y^\\prime_{n/2} & ... & x^\\prime_{n/2}, y^\\prime_{n/2} & ... & x^\\prime_n, y^\\prime_{n/2} \\\\\n",
    "\\vdots   & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ \n",
    "x^\\prime_1, y^\\prime_n & x^\\prime_2, y^\\prime_n & ... & x^\\prime_{n/2}, y^\\prime_n & ... & x^\\prime_n, y^\\prime_n        \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Enter problems**\n",
    "\n",
    "Affine transforms face two problems that must be solved independently:\n",
    "1. **The interpolation problem**: The result of our transformation gives us float coordinates, and we need to decide, for each (i,j), how to assign these coordinates to pixels in the input image.\n",
    "2. **The missing pixel problem**: The result of our transformation may have coordinates which exceed the [-1, 1] range of our original grid and thus fall outside of our original grid.\n",
    "\n",
    "**Solutions to problems**\n",
    "\n",
    "1.  **The interpolation problem**: We will perform a [bilinear interpolation](https://en.wikipedia.org/wiki/Bilinear_interpolation). This takes an average of the values of the pixels corresponding to the four points in the grid surrounding the result of our transformation, with weights depending on how close we are to each of those points. \n",
    "2. **The missing pixel problem**: For these values we need padding, and we face a few options:\n",
    "\n",
    "    1. Adding zeros on the side (so the pixels that fall out will be black)\n",
    "    2. Replacing them by the value at the border\n",
    "    3. Mirroring the content of the picture on the other side (reflect padding).\n",
    "    \n",
    "    \n",
    "### Transformation Method\n",
    "\n",
    "**Zoom**\n",
    "\n",
    "Zoom changes the focus of the image according to a scale. If a scale of >1 is applied, grid pixels will be mapped to coordinates that are more central than the pixel's coordinates (closer to 0,0) while if a scale of <1 is applied, grid pixels will be mapped to more perispheric coordinates (closer to the borders) in the input image.\n",
    "\n",
    "We can also translate our transform to zoom into a non-centrical area of the image. For this we use $col_c$ which displaces the x axis and $row_c$ which displaces the y axis.\n",
    "\n",
    "_Parameters_\n",
    "\n",
    "1. **Scale** How much do we want to zoom in or out to our image.\n",
    "\n",
    "    $S \\in \\mathbb{R}$\n",
    "        \n",
    "2. **Col_pct** How much do we want to displace our zoom along the x axis.\n",
    "\n",
    "    $C_{pct} \\in [0, 1]$\n",
    "    \n",
    "    \n",
    "3. **Row_pct** How much do we want to displace our zoom along the y axis.\n",
    "\n",
    "    $R_{pct} \\in [0, 1]$\n",
    "    \n",
    "\n",
    "<u>Affine matrix</u>\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{scale} & 0 & col_c\\\\\n",
    "0 & \\frac{1}{scale} & row_c\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<u>Transform tensor</u>\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\frac{x_1}{scale}+col_c, \\frac{y_1}{scale}+row_c & \\frac{x_2}{scale}+col_c, \\frac{y_1}{scale}+row_c & ... & \\frac{x_n}{scale}+col_c, \\frac{y_1}{scale}+row_c \\\\\n",
    "\\frac{x_1}{scale}+col_c, \\frac{y_2}{scale}+row_c & \\frac{x_2}{scale}+col_c, \\frac{y_2}{scale}+row_c & ... & \\frac{x_n}{scale}+col_c, \\frac{y_2}{scale}+row_c \\\\\n",
    "\\vdots   & \\vdots & \\vdots & \\vdots \\\\ \n",
    "\\frac{x_1}{scale}+col_c, \\frac{y_n}{scale}+row_c & \\frac{x_2}{scale}+col_c, \\frac{y_n}{scale}+row_c & ... & \\frac{x_n}{scale}+col_c, \\frac{y_n}{scale}+row_c \\\\                                                \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Rotate**\n",
    "\n",
    "Rotate shifts the image around its center in a given angle $\\theta$. The rotation is counterclockwise if $\\theta$ is positive and clockwise if $\\theta$ is negative. If you are curious about the derivation of the rotation matrix you can find it [here](https://matthew-brett.github.io/teaching/rotation_2d.html).\n",
    "\n",
    "_Parameters_\n",
    "\n",
    "1. **Degrees** By which angle do we want to rotate our image.\n",
    "\n",
    "    $D \\in \\mathbb{R}$\n",
    "        \n",
    "<u>Affine matrix</u>\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\theta) & -\\sin(\\theta) & 0\\\\\n",
    "\\sin(\\theta) &  \\cos(\\theta) & 0\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<u>Transform tensor</u>\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\theta)\\cdot{x_1}-\\sin(\\theta)\\cdot{y_1}, \\sin(\\theta)\\cdot{x_1}+\\cos(\\theta)\\cdot{y_1} & \\cos(\\theta)\\cdot{x_2}-\\sin(\\theta)\\cdot{y_1}, \\sin(\\theta)\\cdot{x_2}+\\cos(\\theta)\\cdot{y_1} & ... & \\cos(\\theta)\\cdot{x_n}-\\sin(\\theta)\\cdot{y_1}, \\sin(\\theta)\\cdot{x_n}+\\cos(\\theta)\\cdot{y_1} \\\\\n",
    "\\cos(\\theta)\\cdot{x_1}-\\sin(\\theta)\\cdot{y_2}, \\sin(\\theta)\\cdot{x_1}+\\cos(\\theta)\\cdot{y_2} & \\cos(\\theta)\\cdot{x_2}-\\sin(\\theta)\\cdot{y_2}, \\sin(\\theta)\\cdot{x_2}+\\cos(\\theta)\\cdot{y_2} & ... & \\cos(\\theta)\\cdot{x_n}-\\sin(\\theta)\\cdot{y_2}, \\sin(\\theta)\\cdot{x_n}+\\cos(\\theta)\\cdot{y_2} \\\\\n",
    "\\vdots   & \\vdots & \\vdots & \\vdots \\\\ \n",
    "\\cos(\\theta)\\cdot{x_1}-\\sin(\\theta)\\cdot{y_n}, \\sin(\\theta)\\cdot{x_1}+\\cos(\\theta)\\cdot{y_n} & \\cos(\\theta)\\cdot{x_2}-\\sin(\\theta)\\cdot{y_n}, \\sin(\\theta)\\cdot{x_2}+\\cos(\\theta)\\cdot{y_n} & ... & \\cos(\\theta)\\cdot{x_n}-\\sin(\\theta)\\cdot{y_n}, \\sin(\\theta)\\cdot{x_n}+\\cos(\\theta)\\cdot{y_n} \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def grid_sample_nearest(input, coords, padding_mode='zeros'):\n",
    "    if padding_mode=='border': coords.clamp(-1,1)\n",
    "    bs,ch,h,w = input.size()\n",
    "    sz = torch.tensor([w,h]).float()[None,None]\n",
    "    coords.add_(1).mul_(sz/2)\n",
    "    coords = coords[0].round_().long()\n",
    "    if padding_mode=='zeros':\n",
    "        mask = (coords[...,0] < 0) + (coords[...,1] < 0) + (coords[...,0] >= w) + (coords[...,1] >= h)\n",
    "        mask.clamp_(0,1)\n",
    "    coords[...,0].clamp_(0,w-1)\n",
    "    coords[...,1].clamp_(0,h-1)\n",
    "    result = input[...,coords[...,1],coords[...,0]]\n",
    "    if padding_mode=='zeros': result[...,mask] = result[...,mask].zero_()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def grid_sample(x, coords, mode='bilinear', padding_mode='reflect'):\n",
    "    if mode=='nearest': return grid_sample_nearest(x[None], coords, padding_mode)[0]\n",
    "    if padding_mode=='reflect': # Reflect padding isn't implemented in grid_sample yet\n",
    "        coords[coords < -1] = coords[coords < -1].mul_(-1).add_(-2)\n",
    "        coords[coords > 1] = coords[coords > 1].mul_(-1).add_(2)\n",
    "        padding_mode='zeros'\n",
    "    return F.grid_sample(x[None], coords, mode=mode, padding_mode=padding_mode)[0]\n",
    "\n",
    "def affine_grid(x, matrix, size=None):\n",
    "    if size is None: size = x.size()\n",
    "    elif isinstance(size, int): size=(x.size(0), size, size)\n",
    "    return F.affine_grid(matrix[None,:2], torch.Size((1,)+size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(x, degrees):\n",
    "    angle = degrees * math.pi / 180\n",
    "    return [[cos(angle), -sin(angle), 0.],\n",
    "            [sin(angle),  cos(angle), 0.],\n",
    "            [0.        ,  0.        , 1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = rotate(x, 30)\n",
    "m = x.new_tensor(m)\n",
    "c = affine_grid(x, m)\n",
    "img2 = grid_sample(x, c, padding_mode='zeros')\n",
    "show_image(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def affine_mult(c,m):\n",
    "    size = c.size()\n",
    "    c = c.view(-1,2)\n",
    "    c = torch.addmm(m[:2,2], c,  m[:2,:2].t()) \n",
    "    return c.view(size)\n",
    "\n",
    "def _apply_affine(img, size=None, m=None, func=None, **kwargs):\n",
    "    c = affine_grid(img, torch.eye(3), size=size)\n",
    "    if func is not None: c = func(c, img.size())\n",
    "    if m is not None: c = affine_mult(c, img.new_tensor(m))\n",
    "    return grid_sample(img, c, **kwargs)\n",
    "\n",
    "def apply_affine(m=None, func=None): return partial(_apply_affine, m=m, func=func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom(x, scale: uniform, row_pct = 0.5, col_pct = 0.5):\n",
    "    s = 1-1/scale\n",
    "    col_c = s * (2*col_pct - 1)\n",
    "    row_c = s * (2*row_pct - 1)\n",
    "    return [[1/scale, 0,       col_c],\n",
    "            [0,       1/scale, row_c],\n",
    "            [0,       0,       1.    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(apply_affine(zoom(x, 0.6))(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(apply_affine(zoom(x, 0.6))(x, padding_mode='zeros'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(apply_affine(zoom(x, 2, 0.2, 0.2))(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def affines_mat(matrices=None):\n",
    "    if matrices is None: matrices=[]\n",
    "    matrices = [FloatTensor(m) for m in matrices if m is not None]\n",
    "    return reduce(torch.matmul, matrices, torch.eye(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = apply_affine(rotate(x, 30))(x)\n",
    "img2 = apply_affine(zoom(x, 1.6))(img2)\n",
    "show_image(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = affines_mat([zoom(x,1.6), rotate(x,30)])\n",
    "show_image(apply_affine(c)(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = affines_mat([zoom(x,1.6, 0.8, 0.2), rotate(x,30)])\n",
    "show_image(apply_affine(c)(x, size=48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = affines_mat([zoom(x,1.6, 0.8, 0.2), rotate(x,30)])\n",
    "show_image(apply_affine(c)(x, size=24), hide_axis=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = affines_mat([zoom(x,1.6, 0.8, 0.2), rotate(x,30)])\n",
    "show_image(apply_affine(c)(x, size=48, mode='nearest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = affines_mat([zoom(x,1.6)])\n",
    "show_image(apply_affine(c)(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = affines_mat()\n",
    "show_image(apply_affine(c)(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did with the Lighting transform, we now want to build randomness into our pipeline so we can increase the automatization of the transform process. \n",
    "\n",
    "We will use a uniform distribution for both our transforms since their impact is linear and their domain is $\\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply all transforms**\n",
    "\n",
    "We will build a function called *apply_tfms* which will apply all the transforms to our image. We will make all transforms trying to do as little calculations as possible.\n",
    "\n",
    "We do only one affine transformation by multiplying all the affine matrices of the transforms, then we apply to the coords any non-affine transformation we might want (jitter, elastic distorsion). Next, we crop the coordinates we want to keep and, by doing it before the interpolation, we don't need to compute pixel values that won't be used afterwards. Finally we perform the interpolation and we apply all the transforms that operate pixelwise (brightness, contrast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AffineTransform(Transform):\n",
    "    def __call__(self): return super().__call__()()\n",
    "\n",
    "def dict_groupby(iterable, key=None):\n",
    "    return {k:list(v) for k,v in itertools.groupby(sorted(iterable, key=key), key=key)}\n",
    "\n",
    "def reg_affine(func): return reg_partial(AffineTransform, func)\n",
    "\n",
    "def _apply_tfm_funcs(pixel_func,lighting_func,affine_func,start_func, x,**kwargs):\n",
    "    return pixel_func(lighting_func(affine_func(start_func(x.clone()), **kwargs)))\n",
    "\n",
    "def apply_tfms(tfms):\n",
    "    grouped_tfms = dict_groupby(listify(tfms), lambda o: o.tfm_type)\n",
    "    start_tfms,affine_tfms,coord_tfms,pixel_tfms,lighting_tfms = [\n",
    "        resolve_tfms(grouped_tfms.get(o)) for o in TfmType]\n",
    "    lighting_func = apply_lighting(compose(lighting_tfms))\n",
    "    affine_func = apply_affine(affines_mat(affine_tfms), func=compose(coord_tfms))\n",
    "    return partial(_apply_tfm_funcs,\n",
    "        compose(pixel_tfms),lighting_func,affine_func,compose(start_tfms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@reg_affine\n",
    "def rotate(degrees:uniform) -> TfmType.Affine:\n",
    "    angle = degrees * math.pi / 180\n",
    "    return [[cos(angle), -sin(angle), 0.],\n",
    "            [sin(angle),  cos(angle), 0.],\n",
    "            [0.        ,  0.        , 1.]]\n",
    "\n",
    "def get_zoom_mat(sw, sh, c, r):\n",
    "    return [[sw, 0,  c],\n",
    "            [0, sh,  r],\n",
    "            [0,  0, 1.]]\n",
    "\n",
    "@reg_affine\n",
    "def zoom(scale:uniform=1.0, row_pct:uniform=0.5, col_pct:uniform=0.5) -> TfmType.Affine:\n",
    "    s = 1-1/scale\n",
    "    col_c = s * (2*col_pct - 1)\n",
    "    row_c = s * (2*row_pct - 1)\n",
    "    return get_zoom_mat(1/scale, 1/scale, col_c, row_c)\n",
    "\n",
    "@reg_affine\n",
    "def squish(scale:uniform=1.0, row_pct:uniform=0.5, col_pct:uniform=0.5) -> TfmType.Affine:\n",
    "    if scale <= 1: \n",
    "        col_c = (1-scale) * (2*col_pct - 1)\n",
    "        return get_zoom_mat(scale, 1, col_c, 0.)\n",
    "    else:          \n",
    "        row_c = (1-1/scale) * (2*row_pct - 1)\n",
    "        return get_zoom_mat(1, 1/scale, 0., row_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [rotate_tfm(degrees=(-45,45.), p=0.75),\n",
    "        zoom_tfm(scale=(0.5,2.0), p=0.75)]\n",
    "\n",
    "_,axes = plt.subplots(1,4, figsize=(12,3))\n",
    "for ax in axes: show_image(apply_tfms(tfms)(x), ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [rotate_tfm(degrees=(-45,45.), p=0.75),\n",
    "        zoom_tfm(scale=(1.0,2.0), row_pct=(0,1.), col_pct=(0,1.))]\n",
    "\n",
    "_,axes = plt.subplots(1,4, figsize=(12,3))\n",
    "for ax in axes: show_image(apply_tfms(tfms)(x, size=64, padding_mode='zeros'), ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [0.75,0.9,1.1,1.33]\n",
    "\n",
    "_,axes = plt.subplots(1,4, figsize=(12,3))\n",
    "for i, ax in enumerate(axes): \n",
    "    show_image(apply_affine(affines_mat([squish(scales[i])]))(x, size=64, padding_mode='zeros'), ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [squish_tfm(scale=(0.5,2), row_pct=(0,1.), col_pct=(0,1.))]\n",
    "\n",
    "_,axes = plt.subplots(1,4, figsize=(12,3))\n",
    "for ax in axes: show_image(apply_tfms(tfms)(x, size=64, padding_mode='zeros'), ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coord and pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jitter / flip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two transforms we will use are **jitter** and **flip**. \n",
    "\n",
    "**Jitter**\n",
    "\n",
    "Jitter is a transform which adds a random value to each of the pixels to make them somewhat different than the original ones. In our implementation we first get a random number between (-1, 1) and we multiply it by a constant $M$ which scales it.\n",
    "\n",
    "_Parameters_\n",
    "\n",
    "1. **Magnitude** How much random noise do we want to add to each of the pixels in our image.\n",
    "\n",
    "    $M \\in [0, 1]$\n",
    "    \n",
    "**Flip**\n",
    "\n",
    "Flip is a transform that reflects the image on a given axis.\n",
    "\n",
    "_Parameters_\n",
    "\n",
    "1. **P** Probability of applying the transformation to an input.\n",
    "\n",
    "    $P \\in [0, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@reg_transform\n",
    "def jitter(x, size, magnitude: uniform) -> TfmType.Coord:\n",
    "    return x.add_((torch.rand_like(x)-0.5)*magnitude*2)\n",
    "\n",
    "@reg_transform\n",
    "def flip_lr(x) -> TfmType.Pixel: return x.flip(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = jitter_tfm(magnitude=(0,0.1))\n",
    "\n",
    "_,axes = plt.subplots(1,4, figsize=(12,3))\n",
    "for ax in axes: show_image(apply_tfms(tfm)(x), ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = flip_lr_tfm(p=0.5)\n",
    "\n",
    "_,axes = plt.subplots(1,4, figsize=(12,3))\n",
    "for ax in axes: show_image(apply_tfms(tfm)(x), ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [flip_lr_tfm(p=0.5),\n",
    "        rotate_tfm(degrees=(-45,45.), p=0.5),\n",
    "        zoom_tfm(scale=(0.6,1.6), p=0.8),\n",
    "        contrast_tfm(scale=(0.5,2.0)),\n",
    "        brightness_tfm(change=(0.3,0.7))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(1,4, figsize=(12,3))\n",
    "for ax in axes: show_image(apply_tfms(tfms)(x), ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(2,4, figsize=(12,6))\n",
    "for i in range(4):\n",
    "    tfm = apply_tfms(tfms)\n",
    "    show_image(tfm(x, padding_mode='zeros', size=48), axes[0][i])\n",
    "    show_image(tfm(x, mode='nearest'), axes[1][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomResizedCrop (Torchvision version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compute_zs_mat(sz, scale, squish, invert, row_pct, col_pct):\n",
    "    orig_ratio = math.sqrt(sz[2]/sz[1])\n",
    "    for s,r, i in zip(scale,squish, invert):\n",
    "        s,r = math.sqrt(s),math.sqrt(r)\n",
    "        if s * r <= 1 and s / r <= 1: #Test if we are completely inside the picture\n",
    "            w,h = (s/r, s*r) if i else (s*r,s/r)\n",
    "            w /= orig_ratio\n",
    "            h *= orig_ratio\n",
    "            col_c = (1-w) * (2*col_pct - 1)\n",
    "            row_c = (1-h) * (2*row_pct - 1)\n",
    "            return get_zoom_mat(w, h, col_c, row_c)\n",
    "        \n",
    "    #Fallback, hack to emulate a center crop without cropping anything yet.\n",
    "    if orig_ratio > 1: return get_zoom_mat(1/orig_ratio**2, 1, 0, 0.)\n",
    "    else:              return get_zoom_mat(1, orig_ratio**2, 0, 0.)\n",
    "\n",
    "@reg_transform\n",
    "def zoom_squish(c, sz, scale: uniform = 1.0, squish: uniform=1.0, invert: rand_bool = False, \n",
    "                row_pct:uniform = 0.5, col_pct:uniform = 0.5) -> TfmType.Coord:\n",
    "    #This is intended for scale, squish and invert to be of size 10 (or whatever) so that the transform\n",
    "    #can try a few zoom/squishes before falling back to center crop (like torchvision.RandomResizedCrop)\n",
    "    m = compute_zs_mat(sz, scale, squish, invert, row_pct, col_pct)\n",
    "    return affine_mult(c, FloatTensor(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_resized_crop = zoom_squish_tfm(scale=(0.5,1,10), squish=(0.75,1.33,10), invert=(0.5,10),\n",
    "                                      row_pct=(0,1.), col_pct=(0,1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axes = plt.subplots(2,4, figsize=(12,6))\n",
    "for i in range(4):\n",
    "    tfm = apply_tfms(random_resized_crop)\n",
    "    show_image(tfm(x, size=(3,48,48)), axes[0][i])\n",
    "    show_image(tfm(x, mode='nearest', size=(3,32,32)), axes[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
