{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nb_004 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('data')\n",
    "PATH = DATA_PATH/'cifar10'\n",
    "\n",
    "data_mean,data_std = map(tensor, ([0.491, 0.482, 0.447], [0.247, 0.243, 0.261]))\n",
    "cifar_norm,cifar_denorm = normalize_funcs(data_mean,data_std)\n",
    "\n",
    "train_tfms = [flip_lr(p=0.5),\n",
    "              pad(padding=4),\n",
    "              crop(size=32, row_pct=(0,1.), col_pct=(0,1.))]\n",
    "valid_tfms = []\n",
    "\n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweaks to the OptimWrapper to handle an array of lrs/wds/..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class OptimWrapper():\n",
    "    \"Basic wrapper around an optimizer to simplify HP changes\"\n",
    "    def __init__(self, opt:optim.Optimizer, wd:Floats=0., true_wd:bool=False):\n",
    "        self.opt,self.true_wd = opt,true_wd\n",
    "        self.opt_keys = list(self.opt.param_groups[0].keys())\n",
    "        self.opt_keys.remove('params')\n",
    "        self.read_defaults()\n",
    "        self.wd = wd\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'OptimWrapper over {repr(self.opt)}.\\nTrue weight decay: {self.true_wd}'\n",
    "\n",
    "    #Pytorch optimizer methods\n",
    "    def step(self):\n",
    "        # weight decay outside of optimizer step (AdamW)\n",
    "        if self.true_wd:\n",
    "            for lr,wd,pg in zip(self._lr,self._wd,self.opt.param_groups):\n",
    "                for p in pg['params']: p.data.mul_(1 - wd*lr)\n",
    "            self.set_val('weight_decay', self.listify(0, self._wd))\n",
    "        self.opt.step()\n",
    "    \n",
    "    def zero_grad(self): self.opt.zero_grad()\n",
    "    \n",
    "    #Hyperparameters as properties\n",
    "    @property\n",
    "    def lr(self) -> float: return self._lr[-1]\n",
    "\n",
    "    @lr.setter\n",
    "    def lr(self, val:float): self._lr = self.set_val('lr', self.listify(val, self._lr))\n",
    "    \n",
    "    @property\n",
    "    def mom(self) -> float: return self._mom[-1]\n",
    "\n",
    "    @mom.setter\n",
    "    def mom(self, val:float):\n",
    "        if 'momentum' in self.opt_keys: self.set_val('momentum', self.listify(val, self._mom))\n",
    "        elif 'betas' in self.opt_keys:  self.set_val('betas', (self.listify(val, self._mom), self._beta))\n",
    "        self._mom = self.listify(val, self._mom)\n",
    "    \n",
    "    @property\n",
    "    def beta(self) -> float: return None if self._beta is None else self._beta[-1]\n",
    "\n",
    "    @beta.setter\n",
    "    def beta(self, val:float):\n",
    "        if val is None: return\n",
    "        if 'betas' in self.opt_keys:    self.set_val('betas', (self._mom, self.listify(val, self._beta)))\n",
    "        elif 'alpha' in self.opt_keys:  self.set_val('alpha', self.listify(val, self._beta))\n",
    "        self._beta = self.listify(val, self._beta)\n",
    "    \n",
    "    @property\n",
    "    def wd(self) -> float: return self._wd[-1]\n",
    "\n",
    "    @wd.setter\n",
    "    def wd(self, val:float):\n",
    "        if not self.true_wd: self.set_val('weight_decay', self.listify(val, self._wd))\n",
    "        self._wd = self.listify(val, self._wd)\n",
    "    \n",
    "    #Helper functions\n",
    "    def read_defaults(self):\n",
    "        \"Read the values inside the optimizer for the hyper-parameters\"\n",
    "        self._beta = None\n",
    "        if 'lr' in self.opt_keys: self._lr = self.read_val('lr')\n",
    "        if 'momentum' in self.opt_keys: self._mom = self.read_val('momentum')\n",
    "        if 'alpha' in self.opt_keys: self._beta = self.read_val('alpha')\n",
    "        if 'betas' in self.opt_keys: self._mom,self._beta = self.read_val('betas')\n",
    "        if 'weight_decay' in self.opt_keys: self._wd = self.read_val('weight_decay')\n",
    "    \n",
    "    def set_val(self, key:str, val):\n",
    "        \"Set the values inside the optimizer dictionary at the key\"\n",
    "        if is_tuple(val): val = [(v1,v2) for v1,v2 in zip(*val)]\n",
    "        for v,pg in zip(val,self.opt.param_groups): pg[key] = v\n",
    "        return val\n",
    "    \n",
    "    def read_val(self, key:str) -> Union[List[float],Tuple[List[float],List[float]]]:\n",
    "        \"Read a hyper-parameter key in the optimizer dictionary.\"\n",
    "        val = [pg[key] for pg in self.opt.param_groups]\n",
    "        if is_tuple(val[0]): val = [o[0] for o in val], [o[1] for o in val]\n",
    "        return val\n",
    "    \n",
    "    def listify(self, p, q) -> List[Any]:\n",
    "        \"Wrap listify with an assert.\"\n",
    "        if is_listy(p): assert len(p) == len(q), f'Passing {len(p)} hyperparameters when we have {len(q)} groups.'\n",
    "        return listify(p,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "flatten_model=lambda l: sum(map(flatten_model,l.children()),[]) if len(list(l.children())) else [l]\n",
    "def first_layer(m): return flatten_model(m)[0]\n",
    "\n",
    "def split_model_idx(model, idxs):\n",
    "    \"Split the model according to the indices in [idxs]\"\n",
    "    layers = flatten_model(model)\n",
    "    if idxs[0] != 0: idxs = [0] + idxs\n",
    "    if idxs[-1] != len(layers): idxs.append(len(layers))\n",
    "    return [nn.Sequential(*layers[i:j]) for i,j in zip(idxs[:-1],idxs[1:])]\n",
    "\n",
    "def split_model(model, splits, want_idxs=False):\n",
    "    \"Split the model according to the layers in [splits]\"\n",
    "    layers = flatten_model(model)\n",
    "    idxs = [layers.index(first_layer(s)) for s in listify(splits)]\n",
    "    res = split_model_idx(model, idxs)\n",
    "    return (res,idxs) if want_idxs else res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [model.layers[9], model.layers[15]]\n",
    "layer_groups,idxs = split_model(model, splits, want_idxs=True)\n",
    "lrs = np.array([1e-3,1e-2,0.1])\n",
    "tst_opt = OptimWrapper(optim.SGD([{'params':l.parameters(), 'lr':lr} for l,lr in zip(layer_groups, lrs)]))\n",
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_opt.lr, tst_opt._lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_opt.wd, tst_opt._wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expect exception\n",
    "# tst_opt.wd = [0.1,0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's tweak the learner to handle this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add gradient clipping at this stage in a callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)\n",
    "\n",
    "def set_bn_eval(m):\n",
    "    for l in m.children():\n",
    "        if isinstance(l, bn_types) and not next(l.parameters()).requires_grad:\n",
    "            l.eval()\n",
    "        set_bn_eval(l)\n",
    "\n",
    "@dataclass\n",
    "class BnFreeze(Callback):\n",
    "    learn:Learner\n",
    "    def on_epoch_begin(self, **kwargs): set_bn_eval(self.learn.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def trainable_params(m): return filter(lambda p: p.requires_grad, m.parameters())\n",
    "\n",
    "@dataclass\n",
    "class Learner():\n",
    "    \"Object that wraps together some data, a model, a loss function and an optimizer\"\n",
    "\n",
    "    data:DataBunch\n",
    "    model:nn.Module\n",
    "    opt_fn:Callable=AdamW\n",
    "    loss_fn:Callable=F.cross_entropy\n",
    "    metrics:Collection[Callable]=None\n",
    "    true_wd:bool=True\n",
    "    wd:Floats=1e-6\n",
    "    train_bn:bool=True\n",
    "    path:str = 'models'\n",
    "    callback_fns:Collection[Callable]=None\n",
    "    layer_groups:Collection[nn.Module]=None\n",
    "    def __post_init__(self):\n",
    "        self.path = Path(self.path)\n",
    "        self.path.mkdir(parents=True, exist_ok=True)\n",
    "        self.model = self.model.to(self.data.device)\n",
    "        if not self.layer_groups: self.layer_groups = [self.model]\n",
    "        self.callback_fns = listify(self.callback_fns)\n",
    "        self.callbacks = []\n",
    "\n",
    "    def fit(self, epochs:int, lr:Floats, wd:Floats=None, callbacks:Collection[Callback]=None):\n",
    "        if wd is None: wd = self.wd\n",
    "        self.create_opt(lr, wd)\n",
    "        if callbacks is None: callbacks = []\n",
    "        callbacks += [cb(self) for cb in self.callback_fns]\n",
    "        pbar = master_bar(range(epochs))\n",
    "        self.recorder = Recorder(self.opt, epochs, self.data.train_dl, pbar)\n",
    "        callbacks = [self.recorder] + self.callbacks + callbacks\n",
    "        fit(epochs, self.model, self.loss_fn, self.opt, self.data, callbacks=callbacks, metrics=self.metrics, pbar=pbar)\n",
    "\n",
    "    def create_opt(self, lr:Floats, wd:Floats=0.):\n",
    "        lrs = listify(lr, self.layer_groups)\n",
    "        opt = self.opt_fn([{'params': trainable_params(l), 'lr':lr} for l,lr in zip(self.layer_groups, lrs)])\n",
    "        self.opt = OptimWrapper(opt, wd=wd, true_wd=self.true_wd)\n",
    "\n",
    "        \n",
    "    def split(self, split_on):\n",
    "        if isinstance(split_on,Callable): split_on = split_on(self.model)\n",
    "        self.layer_groups = split_model(self.model, split_on)\n",
    "\n",
    "    def freeze_to(self, n):\n",
    "        for g in self.layer_groups[:n]:\n",
    "            for l in g:\n",
    "                if not self.train_bn or not isinstance(l, bn_types):\n",
    "                    for p in l.parameters(): p.requires_grad = False\n",
    "        for g in self.layer_groups[n:]:\n",
    "            for p in g.parameters(): p.requires_grad = True\n",
    "\n",
    "    def freeze(self):\n",
    "        assert(len(self.layer_groups)>1)\n",
    "        self.freeze_to(-1)\n",
    "        \n",
    "    def unfreeze(self): self.freeze_to(0)\n",
    "        \n",
    "    def save(self, name): torch.save(self.model.state_dict(), self.path/f'{name}.pth')\n",
    "    def load(self, name): self.model.load_state_dict(torch.load(self.path/f'{name}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FilesDataset.from_folder(PATH/'train', classes=['airplane','dog'])\n",
    "valid_ds = FilesDataset.from_folder(PATH/'test', classes=['airplane','dog'])\n",
    "data = DataBunch.create(train_ds, valid_ds, bs=bs, train_tfm=train_tfms, valid_tfm=valid_tfms, num_workers=0, dl_tfms=cifar_norm)\n",
    "len(data.train_dl), len(data.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "learn = Learner(data, model, true_wd=True)\n",
    "learn.metrics = [accuracy]\n",
    "learn.split((model.layers[9],model.layers[15]))\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1,l2 = model.layers[0],model.layers[-1]; l1,l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def requires_grad(l):\n",
    "    p = list(l.parameters())\n",
    "    if not p: return None\n",
    "    return p[0].requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requires_grad(l1[0]),requires_grad(l1[1]),requires_grad(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, lrs, wd=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.opt._lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requires_grad(l1[0]),requires_grad(l1[1]),requires_grad(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, lrs, wd=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "learn = Learner(data, model)\n",
    "learn.metrics = [accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.opt._lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "learn = Learner(data, model)\n",
    "learn.metrics = [accuracy]\n",
    "learn.layer_groups = split_model_idx(model, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, lrs, wd=[1e-4,1e-3,1e-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.opt._wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exception expected\n",
    "# learn.fit(1, lrs, wd=[1e-4,1e-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LRs and WDs are the easiest to pass through the Learner, but if a Callback sets an array of moms or betas, the OptimWrapper will handle them as discriminative moms/betas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See how it fits with the other callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "learn = Learner(data, model)\n",
    "learn.metrics = [accuracy]\n",
    "learn.layer_groups = split_model_idx(model, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find(learn, start_lr=lrs/1000, end_lr=lrs*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = OneCycleScheduler(learn, lrs, 1)\n",
    "learn.fit(1, lrs, callbacks=[sched])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
