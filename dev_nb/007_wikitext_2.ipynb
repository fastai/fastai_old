{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nb_005 import *\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikitext 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset [here](https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip) and unzip it so it's in the folder wikitext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS = '<eos>'\n",
    "PATH=Path('data/wikitext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small helper function to read the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    tokens = []\n",
    "    with open(PATH/filename, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            tokens.append(line.split() + [EOS])\n",
    "    return np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = read_file('wiki.train.tokens')\n",
    "valid_tok = read_file('wiki.valid.tokens')\n",
    "test_tok = read_file('wiki.test.tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36718, 3760, 4358)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tok), len(valid_tok), len(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(train_tok[4][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 113161),\n",
       " (',', 99913),\n",
       " ('.', 73388),\n",
       " ('of', 56889),\n",
       " ('<unk>', 54625),\n",
       " ('and', 50603),\n",
       " ('in', 39453),\n",
       " ('to', 39190),\n",
       " ('<eos>', 36718),\n",
       " ('a', 34237)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = Counter(word for sent in train_tok for word in sent)\n",
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give an id to each token and add the pad token (just in case we need it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = [o for o,c in cnt.most_common()]\n",
    "itos.insert(0,'<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33279"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(itos); vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates the mapping from token to id then numericalizing our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = collections.defaultdict(lambda : 5, {w:i for i,w in enumerate(itos)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = np.array([([stoi[w] for w in s]) for s in train_tok])\n",
    "valid_ids = np.array([([stoi[w] for w in s]) for s in valid_tok])\n",
    "test_ids = np.array([([stoi[w] for w in s]) for s in test_tok])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataLoader():\n",
    "    \"A dataloader that iterates through batches while changing slightly the bptt.\"\n",
    "    def __init__(self, nums, bs, bptt, backwards=False):\n",
    "        self.bs,self.bptt,self.backwards = bs,bptt,backwards\n",
    "        self.data = self.batchify(nums)\n",
    "        self.i,self.iter = 0,0\n",
    "        self.n = len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i,self.iter = 0,0\n",
    "        while self.i < self.n-1 and self.iter<len(self):\n",
    "            if self.i == 0: seq_len = self.bptt + 5 * 5\n",
    "            else:\n",
    "                bptt = self.bptt if np.random.random() < 0.95 else self.bptt / 2.\n",
    "                seq_len = max(5, int(np.random.normal(bptt, 5)))\n",
    "            res = self.get_batch(self.i, seq_len)\n",
    "            self.i += seq_len\n",
    "            self.iter += 1\n",
    "            yield res\n",
    "\n",
    "    def __len__(self): return self.n // self.bptt - 1\n",
    "\n",
    "    def batchify(self, data):\n",
    "        nb = data.shape[0] // self.bs\n",
    "        data = np.array(data[:nb*self.bs])\n",
    "        data = data.reshape(self.bs, -1).T\n",
    "        if self.backwards: data=data[::-1]\n",
    "        return LongTensor(data)\n",
    "\n",
    "    def get_batch(self, i, seq_len):\n",
    "        source = self.data\n",
    "        seq_len = min(seq_len, len(source) - 1 - i)\n",
    "        return source[i:i+seq_len], source[i+1:i+1+seq_len].view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt = 20,10\n",
    "train_dl = LMDataLoader(np.concatenate(train_ids), bs, bptt)\n",
    "valid_dl = LMDataLoader(np.concatenate(valid_ids), bs, bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBunch():\n",
    "    def __init__(self, train_dl, valid_dl, device=None, tfms=None):\n",
    "        self.device = default_device if device is None else device\n",
    "        if not isinstance(train_dl, DeviceDataLoader): \n",
    "            train_dl = DeviceDataLoader(train_dl, self.device, progress_func=tqdm, tfms=tfms)\n",
    "        if not isinstance(valid_dl, DeviceDataLoader): \n",
    "            valid_dl = DeviceDataLoader(valid_dl, self.device, progress_func=tqdm, tfms=tfms)\n",
    "        self.train_dl,self.valid_dl = train_dl,valid_dl\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, train_tfm=None, valid_tfm=None, dl_tfms=None, bs=64, **kwargs):\n",
    "        train_dl = DeviceDataLoader.create(DatasetTfm(train_ds, train_tfm), bs, shuffle=True, \n",
    "                                           tfms=dl_tfms, **kwargs)\n",
    "        valid_dl = DeviceDataLoader.create(DatasetTfm(valid_ds, valid_tfm), bs*2, shuffle=False, \n",
    "                                           tfms=dl_tfms, **kwargs)\n",
    "        return cls(train_dl, valid_dl)\n",
    "        \n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dl.dataset\n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dl.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(train_dl, DeviceDataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(train_dl, valid_dl, default_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use the AWD-LSTM from [Stephen Merity](https://arxiv.org/abs/1708.02182). First, we'll need all different kinds of dropouts. Dropout consists into replacing some coefficients by 0 with probability p. To ensure that the averga of the weights remains constant, we apply a correction to the weights that aren't nullified of a factor `1/(1-p)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_mask(x, sz, p):\n",
    "    \"Returns a dropout mask of the same type as x, size sz, with probability p to cancel an element.\"\n",
    "    return x.new(*sz).bernoulli_(1-p)/(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2., 0., 2., 0., 0., 0., 2., 0., 2.],\n",
       "        [2., 0., 0., 0., 0., 2., 2., 0., 2., 2.],\n",
       "        [0., 2., 2., 0., 0., 2., 0., 0., 0., 2.],\n",
       "        [2., 0., 2., 2., 0., 2., 0., 2., 2., 0.],\n",
       "        [2., 2., 2., 2., 2., 0., 2., 2., 2., 2.],\n",
       "        [2., 2., 0., 2., 0., 2., 0., 0., 0., 0.],\n",
       "        [0., 2., 2., 0., 0., 2., 0., 2., 0., 0.],\n",
       "        [0., 2., 2., 0., 0., 0., 0., 0., 0., 2.],\n",
       "        [0., 2., 0., 0., 0., 2., 2., 2., 2., 0.],\n",
       "        [0., 0., 0., 0., 0., 2., 2., 2., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10,10)\n",
    "dropout_mask(x, (10,10), 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once with have a dropout mask `m`, applying the dropout to `x` is simply done by `x = x * m`. We create our own dropout mask and don't rely on pytorch dropout because we want to nullify the coefficients on the batch dimension but not the token dimension (aka the same coefficients are replaced by zero for each word in the sentence). \n",
    "\n",
    "Inside a RNN, a tensor x will have three dimensions: seq_len, bs, vocab_size, so we create a dropout mask for the last two dimensions and broadcast it to the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p=p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or not self.p: return x\n",
    "        m = dropout_mask(x.data, (1, x.size(1), x.size(2)), self.p)\n",
    "        return m * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.7618, -0.6116,  0.3955,  1.3657,  1.4364, -2.2120,  0.2474,\n",
       "           -0.3893,  0.4495,  1.2473],\n",
       "          [ 0.2402, -0.4327, -1.4691,  0.4653,  0.4280,  0.3873,  0.9973,\n",
       "            0.3174,  0.9244, -1.3500],\n",
       "          [-1.3865,  0.7202, -0.8552,  1.9565,  1.0824, -0.4621, -0.7159,\n",
       "            0.1479,  0.6398,  0.9142],\n",
       "          [ 0.4448, -0.9450,  0.0832,  0.0767,  0.9118,  2.2280,  1.0178,\n",
       "           -0.6625,  0.2360, -0.2408],\n",
       "          [ 0.3941, -1.6472, -0.5152,  0.4187, -0.1898,  0.8900, -0.7480,\n",
       "           -0.9121, -1.7869,  1.0283]],\n",
       " \n",
       "         [[ 0.5170,  0.6714, -0.0522, -0.3161,  1.2824, -0.0983, -1.5031,\n",
       "           -1.8706, -0.1500,  0.9089],\n",
       "          [ 1.3129,  0.7559, -0.2603,  1.1480, -0.0479, -0.7193, -1.2821,\n",
       "           -0.7878,  0.5105,  0.9481],\n",
       "          [ 1.1758, -0.4721, -2.1607, -0.8924,  0.6982,  0.4315, -0.2785,\n",
       "           -0.1542, -1.1626,  0.0448],\n",
       "          [ 2.0093, -0.0582, -1.2177,  0.3014,  0.8134,  0.2001,  0.1971,\n",
       "            0.7769, -0.3051,  0.4290],\n",
       "          [ 0.1914,  0.0329, -1.3884,  1.2543, -0.8739,  0.4197,  2.1740,\n",
       "           -0.4185, -1.1502, -0.1902]]]),\n",
       " tensor([[[-1.5235, -1.2233,  0.7911,  0.0000,  2.8729, -4.4240,  0.4948,\n",
       "           -0.0000,  0.8989,  2.4946],\n",
       "          [ 0.4804, -0.0000, -0.0000,  0.9306,  0.8560,  0.0000,  1.9946,\n",
       "            0.6348,  0.0000, -0.0000],\n",
       "          [-0.0000,  1.4404, -1.7104,  0.0000,  2.1648, -0.0000, -0.0000,\n",
       "            0.0000,  0.0000,  0.0000],\n",
       "          [ 0.8895, -0.0000,  0.1664,  0.1533,  1.8237,  0.0000,  0.0000,\n",
       "           -1.3251,  0.0000, -0.4817],\n",
       "          [ 0.0000, -0.0000, -1.0304,  0.0000, -0.3796,  1.7799, -1.4960,\n",
       "           -1.8241, -0.0000,  2.0566]],\n",
       " \n",
       "         [[ 1.0339,  1.3429, -0.1045, -0.0000,  2.5648, -0.1965, -3.0063,\n",
       "           -0.0000, -0.3000,  1.8179],\n",
       "          [ 2.6258,  0.0000, -0.0000,  2.2959, -0.0958, -0.0000, -2.5642,\n",
       "           -1.5755,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.9442, -4.3213, -0.0000,  1.3964,  0.0000, -0.0000,\n",
       "           -0.0000, -0.0000,  0.0000],\n",
       "          [ 4.0186, -0.0000, -2.4354,  0.6027,  1.6269,  0.0000,  0.0000,\n",
       "            1.5538, -0.0000,  0.8581],\n",
       "          [ 0.0000,  0.0000, -2.7769,  0.0000, -1.7478,  0.8395,  4.3481,\n",
       "           -0.8370, -0.0000, -0.3805]]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_test = RNNDropout(0.5)\n",
    "x = torch.randn(2,5,10)\n",
    "x, dp_test(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightDropout(nn.Module):\n",
    "    \"A module that warps another layer in which some weights will be replaced by 0 during training.\"\n",
    "    \n",
    "    def __init__(self, module, dropout, layer_names=['weight_hh_l0']):\n",
    "        super().__init__()\n",
    "        self.module,self.dropout,self.layer_names = module,dropout,layer_names\n",
    "    \n",
    "    def _setweights(self):\n",
    "        for layer in self.layer_names:\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            w1 = F.dropout(raw_w, p=self.dropout, training=self.training)\n",
    "            module._parameters[layer] = w1\n",
    "            \n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        return self.module.forward(*args)\n",
    "    \n",
    "    def reset(self):\n",
    "        for layer in self.layer_names:\n",
    "            #Makes a copy of the weights of the selected layers.\n",
    "            w = getattr(self.module, layer)\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "        if hasattr(self.module, 'reset'): self.module.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeightDropout(\n",
       "  (module): LSTM(20, 20)\n",
       ")"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = nn.LSTM(20, 20)\n",
    "dp_module = WeightDropout(module, 0.5)\n",
    "dp_module.reset()\n",
    "opt = optim.SGD(dp_module.parameters(), 10)\n",
    "dp_module.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,5,20)\n",
    "x.requires_grad_(requires_grad=True)\n",
    "h = (torch.zeros(1,5,20), torch.zeros(1,5,20))\n",
    "for _ in range(5): x,h = dp_module(x,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0966,  0.1848,  0.0000,  ...,  0.0870,  0.0000,  0.0000],\n",
       "         [-0.0000, -0.0000, -0.3470,  ..., -0.4012,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000, -0.2064,  ..., -0.0000,  0.1284, -0.1940],\n",
       "         ...,\n",
       "         [ 0.0000, -0.0622,  0.2181,  ..., -0.1798, -0.0411,  0.3447],\n",
       "         [ 0.2740,  0.0000, -0.1230,  ...,  0.0000, -0.0000, -0.3689],\n",
       "         [-0.0000, -0.0000,  0.0082,  ..., -0.2975,  0.0668, -0.1271]],\n",
       "        grad_fn=<MulBackward0>), Parameter containing:\n",
       " tensor([[-0.0483,  0.0924,  0.0244,  ...,  0.0435,  0.1158,  0.0845],\n",
       "         [-0.0036, -0.0728, -0.1735,  ..., -0.2006,  0.0105, -0.1592],\n",
       "         [ 0.0174,  0.2131, -0.1032,  ..., -0.1210,  0.0642, -0.0970],\n",
       "         ...,\n",
       "         [ 0.2088, -0.0311,  0.1090,  ..., -0.0899, -0.0205,  0.1723],\n",
       "         [ 0.1370,  0.0754, -0.0615,  ...,  0.0477, -0.0691, -0.1844],\n",
       "         [-0.1813, -0.2008,  0.0041,  ..., -0.1487,  0.0334, -0.0635]],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(dp_module.module, 'weight_hh_l0'),getattr(dp_module,'weight_hh_l0_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.randint(0,20,(10,)).long()\n",
    "loss = F.nll_loss(x.view(-1,20), target)\n",
    "loss.backward()\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, tensor([[-0.0001,  0.0001, -0.0000,  ..., -0.0001,  0.0001,  0.0000],\n",
       "         [ 0.0001,  0.0003, -0.0000,  ...,  0.0004,  0.0002,  0.0001],\n",
       "         [-0.0001,  0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "         ...,\n",
       "         [ 0.0001,  0.0014, -0.0000,  ..., -0.0008,  0.0002, -0.0003],\n",
       "         [-0.0005,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0001],\n",
       "         [-0.0000,  0.0002, -0.0000,  ..., -0.0001,  0.0000, -0.0001]]))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, w_raw = getattr(dp_module.module, 'weight_hh_l0'),getattr(dp_module,'weight_hh_l0_raw')\n",
    "w.grad, w_raw.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0966,  0.1848,  0.0000,  ...,  0.0870,  0.0000,  0.0000],\n",
       "         [-0.0000, -0.0000, -0.3470,  ..., -0.4012,  0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000, -0.2064,  ..., -0.0000,  0.1284, -0.1940],\n",
       "         ...,\n",
       "         [ 0.0000, -0.0622,  0.2181,  ..., -0.1798, -0.0411,  0.3447],\n",
       "         [ 0.2740,  0.0000, -0.1230,  ...,  0.0000, -0.0000, -0.3689],\n",
       "         [-0.0000, -0.0000,  0.0082,  ..., -0.2975,  0.0668, -0.1271]],\n",
       "        grad_fn=<MulBackward0>), Parameter containing:\n",
       " tensor([[-0.0471,  0.0914,  0.0246,  ...,  0.0449,  0.1147,  0.0845],\n",
       "         [-0.0043, -0.0756, -0.1734,  ..., -0.2046,  0.0081, -0.1604],\n",
       "         [ 0.0182,  0.2126, -0.1032,  ..., -0.1206,  0.0640, -0.0966],\n",
       "         ...,\n",
       "         [ 0.2082, -0.0453,  0.1092,  ..., -0.0819, -0.0221,  0.1757],\n",
       "         [ 0.1418,  0.0751, -0.0614,  ...,  0.0478, -0.0686, -0.1859],\n",
       "         [-0.1811, -0.2028,  0.0045,  ..., -0.1474,  0.0329, -0.0629]],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(dp_module.module, 'weight_hh_l0'),getattr(dp_module,'weight_hh_l0_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(nn.Module):\n",
    "\n",
    "    \"Applies dropout in the embedding layer by zeroing out some elements of the embedding vector.\"\n",
    "    def __init__(self, emb):\n",
    "        super().__init__()\n",
    "        self.emb = emb\n",
    "        self.pad_idx = self.emb.padding_idx\n",
    "        if self.pad_idx is None: self.pad_idx = -1\n",
    "\n",
    "    def forward(self, words, dropout=0.1, scale=None):\n",
    "        if dropout:\n",
    "            size = (self.emb.weight.size(0),1)\n",
    "            mask = dropout_mask(self.emb.weight.data, size, dropout)\n",
    "            masked_emb_weight = mask * self.emb.weight\n",
    "        else: masked_emb_weight = self.emb.weight\n",
    "        if scale: masked_emb_weight = scale * masked_emb_weight\n",
    "        return F.embedding(words, masked_emb_weight, self.pad_idx, self.emb.max_norm,\n",
    "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = nn.Embedding(100,20, padding_idx=0)\n",
    "enc_dp = EmbeddingDropout(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0,100,(25,)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
       "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
       "         -0.0000,  0.0000,  0.0000, -0.0000],\n",
       "        [-3.0746,  0.0532, -0.9969, -1.5401, -2.2785, -3.1142, -5.2356, -1.9817,\n",
       "         -0.4740,  0.7455, -0.4058,  1.4690,  0.7676,  0.4037, -0.0132, -4.1723,\n",
       "          0.8027, -1.0026, -1.9151,  2.5202],\n",
       "        [-2.6915, -1.3094,  2.5535, -2.0829,  1.0566, -2.4929,  0.1980,  2.1521,\n",
       "         -1.9781,  0.4168, -1.6336,  1.8876,  1.8933,  2.5815,  1.3373,  0.4616,\n",
       "         -1.5860,  2.5099,  2.6191,  0.9438],\n",
       "        [ 0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.0000,  0.0000,  0.0000, -0.0000],\n",
       "        [ 0.2953,  1.9750,  3.3688,  2.1726, -1.6868,  0.7382,  0.7309,  2.3460,\n",
       "         -3.7565, -0.3633, -0.3705,  1.0253,  2.0195,  0.2571, -1.9339, -2.8782,\n",
       "          3.3939,  2.9073,  0.4097,  1.9072],\n",
       "        [ 0.5697, -0.8794, -1.1051,  0.7381,  3.2064,  1.1869,  0.6103,  4.0163,\n",
       "         -2.8680,  0.2569, -0.1953,  1.7844, -0.6983, -3.0204,  0.1995,  4.6198,\n",
       "         -2.4291,  0.9621,  2.2416,  0.8460],\n",
       "        [ 1.1072, -0.7850, -1.2569, -1.8064, -0.2758,  1.5919,  2.1156,  0.8011,\n",
       "          3.1572,  0.0501, -1.6282,  2.1985, -0.1718,  1.1339, -1.5130,  2.3161,\n",
       "          5.8747,  2.9110,  4.0537,  0.1406],\n",
       "        [-0.6216, -2.1223,  0.2563, -1.2750, -2.4240, -0.9206, -0.4679,  1.0163,\n",
       "          0.4720, -2.3387,  0.6266, -1.5123,  2.9876,  2.5822,  0.8812,  0.4616,\n",
       "          2.7637,  3.2318,  0.5398, -0.5174],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
       "         -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
       "          0.0000, -0.0000,  0.0000, -0.0000],\n",
       "        [ 0.2953,  1.9750,  3.3688,  2.1726, -1.6868,  0.7382,  0.7309,  2.3460,\n",
       "         -3.7565, -0.3633, -0.3705,  1.0253,  2.0195,  0.2571, -1.9339, -2.8782,\n",
       "          3.3939,  2.9073,  0.4097,  1.9072],\n",
       "        [ 0.3960, -0.7506,  2.3170,  1.6889,  2.0988,  1.9712, -1.8320,  5.1785,\n",
       "         -0.7022, -0.7580, -2.3049, -0.6902,  2.4481, -0.8291, -0.9982,  0.9262,\n",
       "         -1.8147,  2.5985,  2.0517,  1.0900],\n",
       "        [ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
       "         -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.0000, -0.0000, -0.0000, -0.0000],\n",
       "        [ 0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
       "         -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
       "          0.0000, -0.0000,  0.0000, -0.0000],\n",
       "        [-0.3157,  0.0871,  1.0549,  1.5682, -0.6026, -0.4858,  2.3007, -0.3649,\n",
       "          4.2785, -1.6199,  0.1949, -1.9335, -2.0696,  0.4727, -1.9461, -1.0550,\n",
       "         -0.5563, -2.6314, -0.6478, -1.6041],\n",
       "        [ 0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
       "         -0.0000, -0.0000,  0.0000, -0.0000],\n",
       "        [-0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
       "         -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
       "          0.0000, -0.0000,  0.0000,  0.0000],\n",
       "        [-0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
       "          0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
       "          0.0000, -0.0000,  0.0000,  0.0000],\n",
       "        [-1.4331,  1.2885, -1.8112, -3.6487,  1.5365,  2.2097, -0.1210, -4.0178,\n",
       "         -0.9067,  0.2416,  1.7297,  1.0364,  0.1152,  2.4813,  0.8581,  0.7856,\n",
       "         -0.6928, -0.6553,  0.0081,  0.1336],\n",
       "        [-0.3157,  0.0871,  1.0549,  1.5682, -0.6026, -0.4858,  2.3007, -0.3649,\n",
       "          4.2785, -1.6199,  0.1949, -1.9335, -2.0696,  0.4727, -1.9461, -1.0550,\n",
       "         -0.5563, -2.6314, -0.6478, -1.6041],\n",
       "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
       "          0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
       "          0.0000, -0.0000, -0.0000,  0.0000],\n",
       "        [-1.6983, -2.0710, -2.8568, -0.2215, -2.4582,  1.1068,  3.1352, -2.2469,\n",
       "         -4.7432,  2.0595,  0.4809,  1.3533, -3.3434, -0.4067,  1.3954, -2.2860,\n",
       "          0.2896, -2.1592,  0.3023,  0.2292],\n",
       "        [-0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
       "          0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 2.2118, -1.7264,  0.1159, -0.5105, -1.1614, -1.7116, -0.4231,  0.1977,\n",
       "          0.1689, -3.6297,  4.3669, -0.0019, -2.1002,  0.5867,  1.3009, -2.6246,\n",
       "          1.1866,  0.1172,  1.0763, -2.1137],\n",
       "        [-0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "         -0.0000, -0.0000,  0.0000, -0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
       "          0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
       "          0.0000, -0.0000, -0.0000,  0.0000]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_dp(x, dropout=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. AWD-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_var(h):\n",
    "    \"Detaches h from its history.\"\n",
    "    return h.detach() if type(h) == torch.Tensor else tuple(repackage_var(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCore(nn.Module):\n",
    "    \"AWD-LSTM/QRNN inspired by https://arxiv.org/abs/1708.02182\"\n",
    "\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token, bidir=False,\n",
    "                 dropouth=0.3, dropouti=0.65, dropoute=0.1, wdrop=0.5, qrnn=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.bs,self.qrnn,self.ndir = 1, qrnn,(2 if bidir else 1)\n",
    "        self.emb_sz,self.n_hid,self.n_layers,self.dropoute = emb_sz,n_hid,n_layers,dropoute\n",
    "        self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.dp_encoder = EmbeddingDropout(self.encoder)\n",
    "        if self.qrnn:\n",
    "            #Using QRNN requires cupy: https://github.com/cupy/cupy\n",
    "            from .torchqrnn.qrnn import QRNNLayer\n",
    "            self.rnns = [QRNNLayer(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                save_prev_x=True, zoneout=0, window=2 if l == 0 else 1, output_gate=True) for l in range(n_layers)]\n",
    "            if wdrop:\n",
    "                for rnn in self.rnns:\n",
    "                    rnn.linear = WeightDrop(rnn.linear, wdrop, layer_names=['weight'])\n",
    "        else:\n",
    "            self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                1, bidirectional=bidir) for l in range(n_layers)]\n",
    "            if wdrop: self.rnns = [WeightDrop(rnn, wdrop) for rnn in self.rnns]\n",
    "        self.rnns = torch.nn.ModuleList(self.rnns)\n",
    "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.dropouti = RNNDropout(dropouti)\n",
    "        self.dropouths = nn.ModuleList([RNNDropout(dropouth) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        sl,bs = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        emb = self.dp_encoder(input, dropout=self.dropoute if self.training else 0)\n",
    "        emb = self.dropouti(emb)\n",
    "        raw_output = emb\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,drop) in enumerate(zip(self.rnns, self.dropouths)):\n",
    "            #with warnings.catch_warnings():\n",
    "            #    warnings.simplefilter(\"ignore\")\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = drop(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "        self.hidden = repackage_var(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def one_hidden(self, l):\n",
    "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz)//self.ndir\n",
    "        return self.weights.new(self.ndir, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        [r.reset() for r in self.rnns if hasattr(r, 'reset')]\n",
    "        self.weights = next(self.parameters()).data\n",
    "        if self.qrnn: self.hidden = [self.one_hidden(l) for l in range(self.n_layers)]\n",
    "        else: self.hidden = [(self.one_hidden(l), self.one_hidden(l)) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDecoder(nn.Module):\n",
    "    \"To go on top of a RNN_Core module\"\n",
    "    \n",
    "    initrange=0.1\n",
    "    \n",
    "    def __init__(self, n_out, n_hid, dropout, tie_encoder=None, bias=True):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.dropout = RNNDropout(dropout)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.dropout(outputs[-1])\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialRNN(nn.Sequential):\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, tie_weights=True, qrnn=False, bias=True,\n",
    "                 dropout=0.4, dropouth=0.3, dropouti=0.5, dropoute=0.1, wdrop=0.5):\n",
    "    \"To create a full AWD-LSTM\"\n",
    "    rnn_enc = RNNCore(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token, qrnn=qrnn,\n",
    "                 dropouth=dropouth, dropouti=dropouti, dropoute=dropoute, wdrop=wdrop)\n",
    "    enc = rnn_enc.encoder if tie_weights else None\n",
    "    return SequentialRNN(rnn_enc, LinearDecoder(vocab_sz, emb_sz, dropout, tie_encoder=enc, bias=bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_model = get_language_model(500, 20, 100, 2, 0, wdrop=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, 500, (10,5)).long()\n",
    "z = tst_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Callbacks to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RNNTrainer(Callback):\n",
    "    learn:Learner\n",
    "    bptt:int\n",
    "    alpha:float=0.\n",
    "    beta:float=0.\n",
    "    \n",
    "    def on_loss_begin(self, last_output, **kwargs):\n",
    "        #Save the extra outputs for later and only returns the true output.\n",
    "        self.raw_out,self.out = last_output[1],last_output[2]\n",
    "        return last_output[0]\n",
    "    \n",
    "    def on_backward_begin(self, last_loss, last_input, last_output, **kwargs):\n",
    "        #Adjusts the lr to the bptt selected\n",
    "        self.learn.opt.lr *= last_input.size(0) / self.bptt\n",
    "        #AR and TAR\n",
    "        if self.alpha != 0.:  last_loss += (self.alpha * self.out[-1].pow(2).mean()).sum()\n",
    "        if self.beta != 0.:\n",
    "            h = self.raw_out[-1]\n",
    "            if len(h)>1: last_loss += (self.beta * (h[1:] - h[:-1]).pow(2).mean()).sum()\n",
    "        return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sylvain\\Anaconda3\\envs\\fastai1\\lib\\site-packages\\torch\\backends\\cudnn\\__init__.py:89: UserWarning: PyTorch was compiled without cuDNN support. To use cuDNN, rebuild PyTorch making sure the library is visible to the build system.\n",
      "  \"PyTorch was compiled without cuDNN support. To use cuDNN, rebuild \"\n"
     ]
    }
   ],
   "source": [
    "model = get_language_model(vocab_size, 20, 100, 2, 0, wdrop=0.)\n",
    "learn = Learner(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e794dd1c848b4ab6a8dda90328fd35e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3137259c40844837ba708a52c6bd001d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10442), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-166-f25d71b354e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRNNTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbptt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Work\\Deeplearning\\fastai_v1\\dev_nb\\nb_004a.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Work\\Deeplearning\\fastai_v1\\dev_nb\\nb_004.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(epochs, model, loss_fn, opt, data, callbacks, metrics)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m             \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Work\\Deeplearning\\fastai_v1\\dev_nb\\nb_004.py\u001b[0m in \u001b[0;36mloss_batch\u001b[1;34m(model, xb, yb, loss_fn, opt, cb_handler, metrics)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcb_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCallbackHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai1\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-89-2eb3b9311abe>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0memb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdp_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropoute\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0memb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropouti\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mraw_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mnew_hidden\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mraw_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-3f2a2473e706>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdropout_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-81854acbf64d>\u001b[0m in \u001b[0;36mdropout_mask\u001b[1;34m(x, sz, p)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdropout_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"Returns a dropout mask of the same type as x, size sz, with probability p to cancel an element.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbernoulli_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cb = RNNTrainer(learn, bptt, alpha=2, beta=1)\n",
    "learn.opt_fn = partial(optim.Adam)\n",
    "learn.fit(1, 0.1, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
