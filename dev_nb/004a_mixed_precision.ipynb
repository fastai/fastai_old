{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nb_004 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('data')\n",
    "PATH = DATA_PATH/'cifar10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mean,data_std = map(tensor, ([0.491, 0.482, 0.447], [0.247, 0.243, 0.261]))\n",
    "cifar_norm = normalize_tfm(mean=data_mean,std=data_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = [flip_lr_tfm(p=0.5),\n",
    "              pad_tfm(padding=4),\n",
    "              crop_tfm(size=32, row_pct=(0,1.), col_pct=(0,1.)),\n",
    "              cifar_norm]\n",
    "valid_tfms = [cifar_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_half(b):  return [b[0].half(), b[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class DeviceDataLoader():\n",
    "    dl: DataLoader\n",
    "    device: torch.device\n",
    "    progress_func: Callable\n",
    "    half: bool = False\n",
    "        \n",
    "    def __len__(self): return len(self.dl)\n",
    "    def __iter__(self):\n",
    "        self.gen = (to_device(self.device,o) for o in self.dl)\n",
    "        if self.half: self.gen = (to_half(o) for o in self.gen)\n",
    "        if self.progress_func is not None:\n",
    "            self.gen = self.progress_func(self.gen, total=len(self.dl), leave=False)\n",
    "        return iter(self.gen)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, *args, device=default_device, progress_func=tqdm, **kwargs):\n",
    "        return cls(DataLoader(*args, **kwargs), device=device, progress_func=progress_func, half=False)\n",
    "\n",
    "nb_002b.DeviceDataLoader = DeviceDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FilesDataset.from_folder(PATH/'train', classes=['airplane','dog'])\n",
    "valid_ds = FilesDataset.from_folder(PATH/'test', classes=['airplane','dog'])\n",
    "data = DataBunch.create(train_ds, valid_ds, bs=bs, train_tfm=train_tfms, valid_tfm=valid_tfms, num_workers=4)\n",
    "len(data.train_dl), len(data.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "learn = Learner(data, model)\n",
    "learn.metrics = [accuracy]\n",
    "sched = OneCycleScheduler(learn, 0.1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bn2float(module):\n",
    "    if isinstance(module, torch.nn.modules.batchnorm._BatchNorm): module.float()\n",
    "    for child in module.children(): bn2float(child)\n",
    "    return module\n",
    "\n",
    "def model2half(model):\n",
    "    \"Converts the model to half precision except the batchnorm layers\"\n",
    "    return bn2float(model.half())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to save the master model in FP32 with flat tensors (apparently it helps with performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch._utils import _unflatten_dense_tensors\n",
    "from torch.nn.utils import parameters_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import vector_to_parameters\n",
    "from torch.nn.utils.convert_parameters import _check_param_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_parameters1(vec, parameters):\n",
    "    \n",
    "    if not isinstance(vec, torch.Tensor):\n",
    "        raise TypeError('expected torch.Tensor, but got: {}'\n",
    "                        .format(torch.typename(vec)))\n",
    "    param_device = None\n",
    "    pointer = 0\n",
    "    for param in parameters:\n",
    "        param_device = _check_param_device(param, param_device)\n",
    "        num_param = torch.prod(torch.LongTensor(list(param.size())))\n",
    "        param.data.copy_(vec[pointer:pointer + num_param].view(param.size()).data)\n",
    "        pointer += num_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_master(model, flat_master=False):\n",
    "    \"Returns two lists, one for the model parameters in FP16 and one for the master parameters in FP32\"\n",
    "    model_params = [param for param in model.parameters() if param.requires_grad]\n",
    "    if flat_master:\n",
    "        master_params = parameters_to_vector([param.data.float() for param in model_params])\n",
    "        master_params = torch.nn.Parameter(master_params)\n",
    "        master_params.requires_grad = True\n",
    "        if master_params.grad is None: master_params.grad = master_params.new(*master_params.size())\n",
    "        return model_params, [master_params]\n",
    "    else:\n",
    "        master_params = [param.clone().float().detach() for param in model_params]\n",
    "        for param in master_params: param.requires_grad = True\n",
    "        return model_params, master_params\n",
    "\n",
    "#export\n",
    "def model_g2master_g(model_params, master_params, flat_master=False):\n",
    "    \"Copies the model gradients to the master parameters for the optimizer step\"\n",
    "    if flat_master:\n",
    "        master_params[0].grad.data.copy_(parameters_to_vector([p.grad.data.float() for p in model_params]))\n",
    "    else:\n",
    "        for model, master in zip(model_params, master_params):\n",
    "            if model.grad is not None:\n",
    "                if master.grad is None: master.grad = master.data.new(*master.data.size())\n",
    "                master.grad.data.copy_(model.grad.data)\n",
    "            else: master.grad = None\n",
    "\n",
    "#export\n",
    "def master2model(model_params, master_params, flat_master=False):\n",
    "    \"Copy master parameters to model parameters\"\n",
    "    if flat_master:\n",
    "        for model, master in zip(model_params, _unflatten_dense_tensors(master_params[0].data, model_params)):\n",
    "            model.data.copy_(master)\n",
    "    else: \n",
    "        for model, master in zip(model_params, master_params): model.data.copy_(master.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@dataclass\n",
    "class MixedPrecision(Callback):\n",
    "    learn:Learner\n",
    "    loss_scale:int = 512\n",
    "    flat_master:bool = False\n",
    "    def __post_init__(self): assert torch.backends.cudnn.enabled, \"Mixed precision training requires cudnn.\" \n",
    "    \n",
    "    def on_train_begin(self, **kwargs):\n",
    "        #Insures the dataloaders are in half precision.\n",
    "        self.learn.data.train_dl.half = True\n",
    "        if hasattr(self.learn.data, 'valid_dl') and self.learn.data.valid_dl is not None:\n",
    "            self.learn.data.valid_dl.half = True\n",
    "        #Get a copy of the model params in FP32\n",
    "        self.model_params, self.master_params = get_master(self.learn.model, self.flat_master)\n",
    "        #Changes the optimizer so that the optimization step is done in FP32.\n",
    "        opt = self.learn.opt\n",
    "        mom,wd,beta = opt.mom,opt.wd,opt.beta\n",
    "        self.learn.opt.opt = self.learn.opt_fn(self.master_params, self.learn.opt.lr)\n",
    "        opt.mom,opt.wd,opt.beta = mom,wd,beta\n",
    "    \n",
    "    def on_loss_begin(self, last_output, **kwargs):\n",
    "        #It's better to compute the loss in FP32, to avoid reduction overflow.\n",
    "        return last_output.float()\n",
    "    \n",
    "    def on_backward_begin(self, last_loss, **kwargs):\n",
    "        #To avoid gradient underflow, we scale the gradients\n",
    "        return last_loss * self.loss_scale\n",
    "    \n",
    "    def on_backward_end(self, **kwargs):\n",
    "        #Convert the gradients back to FP32 and divide them by the scale.\n",
    "        model_g2master_g(self.model_params, self.master_params, self.flat_master)\n",
    "        for param in self.master_params: param.grad.div_(self.loss_scale)\n",
    "    \n",
    "    def on_step_end(self, **kwargs):\n",
    "        #Zeros the gradients of the model since the optimizer is disconnected.\n",
    "        self.learn.model.zero_grad()\n",
    "        #Update the params from master to model.\n",
    "        master2model(self.model_params, self.master_params, self.flat_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "model = model2half(model)\n",
    "learn = Learner(data, model)\n",
    "learn.metrics = [accuracy]\n",
    "scheds = [MixedPrecision(learn, flat_master=True), OneCycleScheduler(learn, 0.1, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, 1e-2, callbacks=scheds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheds[0].master_params[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
