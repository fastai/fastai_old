{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nb_004a import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('data')\n",
    "PATH = DATA_PATH/'cifar10'\n",
    "\n",
    "data_mean,data_std = map(tensor, ([0.491, 0.482, 0.447], [0.247, 0.243, 0.261]))\n",
    "cifar_norm,cifar_denorm = normalize_funcs(data_mean.half(),data_std.half())\n",
    "\n",
    "train_tfms = [flip_lr_tfm(p=0.5),\n",
    "              pad_tfm(padding=4),\n",
    "              crop_tfm(size=32, row_pct=(0,1.), col_pct=(0,1.))]\n",
    "valid_tfms = []\n",
    "\n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_half(b):  return [b[0].half(), b[1]]\n",
    "\n",
    "@dataclass\n",
    "class DeviceDataLoader():\n",
    "    dl: DataLoader\n",
    "    device: torch.device\n",
    "    progress_func:Callable=None\n",
    "    tfms: List[Callable]=None\n",
    "    half: bool = False\n",
    "\n",
    "    def __len__(self): return len(self.dl)\n",
    "\n",
    "    def proc_batch(self,b):\n",
    "        b = to_device(self.device,b)\n",
    "        if self.half: b = to_half(b)\n",
    "        return b if self.tfms is None else self.tfms(b)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.gen = map(self.proc_batch, self.dl)\n",
    "        if self.progress_func is not None:\n",
    "            self.gen = self.progress_func(self.gen, total=len(self.dl), leave=False)\n",
    "        return iter(self.gen)\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, *args, device=default_device, progress_func=tqdm, tfms=tfms, **kwargs):\n",
    "        return cls(DataLoader(*args, **kwargs), device=device, progress_func=progress_func, tfms=tfms, half=False)\n",
    "\n",
    "import nb_003b\n",
    "nb_003b.DeviceDataLoader = DeviceDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FilesDataset.from_folder(PATH/'train', classes=['airplane','dog'])\n",
    "valid_ds = FilesDataset.from_folder(PATH/'test', classes=['airplane','dog'])\n",
    "data = DataBunch.create(train_ds, valid_ds, bs=bs, num_workers=4, \n",
    "                        train_tfm=train_tfms, valid_tfm=valid_tfms, dl_tfms=cifar_norm)\n",
    "len(data.train_dl), len(data.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "learn = Learner(data, model)\n",
    "learn.metrics = [accuracy]\n",
    "sched = OneCycleScheduler(learn, 0.1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bn2float(module):\n",
    "    if isinstance(module, torch.nn.modules.batchnorm._BatchNorm): module.float()\n",
    "    for child in module.children(): bn2float(child)\n",
    "    return module\n",
    "\n",
    "def model2half(model):\n",
    "    \"Converts the model to half precision except the batchnorm layers\"\n",
    "    return bn2float(model.half())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to save the master model in FP32 with flat tensors (apparently it helps with performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch._utils import _unflatten_dense_tensors\n",
    "from torch.nn.utils import parameters_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_parameters1(vec, parameters):\n",
    "    \n",
    "    if not isinstance(vec, torch.Tensor):\n",
    "        raise TypeError('expected torch.Tensor, but got: {}'\n",
    "                        .format(torch.typename(vec)))\n",
    "    param_device = None\n",
    "    pointer = 0\n",
    "    for param in parameters:\n",
    "        param_device = _check_param_device(param, param_device)\n",
    "        num_param = torch.prod(torch.LongTensor(list(param.size())))\n",
    "        param.data.copy_(vec[pointer:pointer + num_param].view(param.size()).data)\n",
    "        pointer += num_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_master(layer_groups:Collection[nn.Module], flat_master:bool=False) -> Tuple[List[List[Tensor]], List[List[Tensor]]]:\n",
    "    \"Returns two lists, one for the model parameters in FP16 and one for the master parameters in FP32\"\n",
    "    model_params = [[param for param in lg.parameters() if param.requires_grad] for lg in layer_groups]\n",
    "    if flat_master:\n",
    "        master_params = [parameters_to_vector([param.data.float() for param in lg]) for lg in model_params]\n",
    "        master_params = [torch.nn.Parameter(mp, requires_grad=True) for mp in master_params]\n",
    "        for mp in master_params:\n",
    "            if mp.grad is None: mp.grad = mp.new(*mp.size())\n",
    "        return model_params, [[mp] for mp in master_params]\n",
    "    else:\n",
    "        master_params = [[param.clone().float().detach() for param in lg] for lg in model_params]\n",
    "        for mp in master_params:\n",
    "            for param in mp: param.requires_grad = True\n",
    "        return model_params, master_params\n",
    "\n",
    "def model_g2master_g(model_params:Sequence[Tensor], master_params:Sequence[Tensor], flat_master:bool=False):\n",
    "    \"Copies the model gradients to the master parameters for the optimizer step\"\n",
    "    if flat_master:\n",
    "        for model_group,master_group in zip(model_params,master_params):\n",
    "            master_group[0].grad.data.copy_(parameters_to_vector([p.grad.data.float() for p in model_group]))\n",
    "    else:\n",
    "        for model_group,master_group in zip(model_params,master_params):\n",
    "            for model, master in zip(model_group, master_group):\n",
    "                if model.grad is not None:\n",
    "                    if master.grad is None: master.grad = master.data.new(*master.data.size())\n",
    "                    master.grad.data.copy_(model.grad.data)\n",
    "                else: master.grad = None\n",
    "\n",
    "def master2model(model_params:Sequence[Tensor], master_params:Sequence[Tensor], flat_master:bool=False):\n",
    "    \"Copy master parameters to model parameters\"\n",
    "    if flat_master:\n",
    "        for model_group,master_group in zip(model_params,master_params):\n",
    "            for model, master in zip(model_group, _unflatten_dense_tensors(master_group[0].data, model_group)):\n",
    "                model.data.copy_(master)\n",
    "    else:\n",
    "        for model_group,master_group in zip(model_params,master_params):\n",
    "            for model, master in zip(model_group, master_group): model.data.copy_(master.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch._utils import _unflatten_dense_tensors\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "\n",
    "@dataclass\n",
    "class MixedPrecision(Callback):\n",
    "    \"Callback that handles mixed-precision training\"\n",
    "    learn:Learner\n",
    "    loss_scale:float=512.\n",
    "    flat_master:bool=False\n",
    "    def __post_init__(self): assert torch.backends.cudnn.enabled, \"Mixed precision training requires cudnn.\" \n",
    "    \n",
    "    def on_train_begin(self, **kwargs):\n",
    "        #Insures the dataloaders are in half precision.\n",
    "        self.learn.data.train_dl.half = True\n",
    "        if hasattr(self.learn.data, 'valid_dl') and self.learn.data.valid_dl is not None:\n",
    "            self.learn.data.valid_dl.half = True\n",
    "        #Get a copy of the model params in FP32\n",
    "        self.model_params, self.master_params = get_master(self.learn.layer_groups, self.flat_master)\n",
    "        #Changes the optimizer so that the optimization step is done in FP32.\n",
    "        opt = self.learn.opt\n",
    "        mom,wd,beta = opt.mom,opt.wd,opt.beta\n",
    "        opt_params = [{'params': mp, 'lr': lr} for mp,lr in zip(self.master_params, self.learn.opt._lr)]\n",
    "        self.learn.opt.opt = self.learn.opt_fn(opt_params)\n",
    "        opt.mom,opt.wd,opt.beta = mom,wd,beta\n",
    "    \n",
    "    def on_loss_begin(self, last_output:Tensor, **kwargs) -> Tensor:\n",
    "        #It's better to compute the loss in FP32, to avoid reduction overflow.\n",
    "        return last_output.float()\n",
    "    \n",
    "    def on_backward_begin(self, last_loss:Rank0Tensor, **kwargs) -> Rank0Tensor:\n",
    "        #To avoid gradient underflow, we scale the gradients\n",
    "        return last_loss * self.loss_scale\n",
    "    \n",
    "    def on_backward_end(self, **kwargs):\n",
    "        #Convert the gradients back to FP32 and divide them by the scale.\n",
    "        model_g2master_g(self.model_params, self.master_params, self.flat_master)\n",
    "        for group in self.master_params:\n",
    "            for param in group: param.grad.div_(self.loss_scale)\n",
    "    \n",
    "    def on_step_end(self, **kwargs):\n",
    "        #Zeros the gradients of the model since the optimizer is disconnected.\n",
    "        self.learn.model.zero_grad()\n",
    "        #Update the params from master to model.\n",
    "        master2model(self.model_params, self.master_params, self.flat_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "model = model2half(model)\n",
    "learn = Learner(data, model)\n",
    "learn.metrics = [accuracy]\n",
    "scheds = [MixedPrecision(learn, flat_master=True), OneCycleScheduler(learn, 0.1, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, 1e-2, callbacks=scheds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model.layers[0][0].weight.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheds[0].master_params[0][0].size(),scheds[0].master_params[0][0].type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with discriminative lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Darknet([1, 2, 2, 2, 2], num_classes=2, nf=16)\n",
    "model = model2half(model)\n",
    "layer_groups = split_model(model.layers, [5,9])\n",
    "learn = Learner(data, model)\n",
    "learn.layer_groups = layer_groups\n",
    "learn.metrics = [accuracy]\n",
    "scheds = [MixedPrecision(learn, flat_master=True), OneCycleScheduler(learn, 0.1, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, 1e-2, callbacks=scheds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model.layers[0][0].weight.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for master in scheds[0].master_params:\n",
    "    print(master[0].size(),master[0].type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
