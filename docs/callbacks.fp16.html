---
title: callbacks.fp16
keywords: 
sidebar: home_sidebar
tags: 
summary: Callback support for half precision (fp16) training. Increases training speed.
---

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="callbacks.fp16">callbacks.fp16<a class="anchor-link" href="#callbacks.fp16">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Type an introduction of the package here.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Global-Variable-Definitions:">Global Variable Definitions:<a class="anchor-link" href="#Global-Variable-Definitions:">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4><a id=get_master></a><code>get_master</code></h4>
<p>(<code>layer_groups</code>:<code>Collection</code>[<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module"><code>Module</code></a>], <code>flat_master</code>:<code>bool</code>=<code>False</code>) -&gt; <code>Tuple</code>[<code>List</code>[<code>List</code>[<code>Tensor</code>]], <code>List</code>[<code>List</code>[<code>Tensor</code>]]]<div style="text-align: right"><a href="https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L9">[source]</a></div></p>
<p>Returns two lists, one for the model parameters in FP16 and one for the master parameters in FP32</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="http://docs.fast.ai/callbacks.fp16.html#get_master"><code>get_master</code></a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4><a id=master2model></a><code>master2model</code></h4>
<p>(<code>model_params</code>:<code>Sequence</code>[<code>Tensor</code>], <code>master_params</code>:<code>Sequence</code>[<code>Tensor</code>], <code>flat_master</code>:<code>bool</code>=<code>False</code>) -&gt; <code>NoneType</code><div style="text-align: right"><a href="https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L43">[source]</a></div></p>
<p>Copy master parameters to model parameters</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="http://docs.fast.ai/callbacks.fp16.html#master2model"><code>master2model</code></a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h3><a id=MixedPrecision></a><em>class</em> <code>MixedPrecision</code></h3>
<p>(<code>learn</code>:<a href="fastai.basic_train.html#Learner"><code>Learner</code></a>, <code>loss_scale</code>:<code>float</code>=<code>512.0</code>, <code>flat_master</code>:<code>bool</code>=<code>False</code>) -&gt; <code>NoneType</code> :: Inherits (<a href="fastai.callback.html#Callback"><code>Callback</code></a>)<div style="text-align: right"><a href="https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L55">[source]</a></div></p>
<p>Callback that handles mixed-precision training</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="http://docs.fast.ai/callbacks.fp16.html#MixedPrecision"><code>MixedPrecision</code></a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4><a id=on_backward_begin></a><code>on_backward_begin</code></h4>
<p>(<code>last_loss</code>:<code>OneEltTensor</code>, <code>kwargs</code>:<code>Any</code>) -&gt; <code>OneEltTensor</code><div style="text-align: right"><a href="https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L89">[source]</a></div></p>
<p>Scale gradients up by <code>loss_scale</code> to prevent underflow</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>MixedPrecision.on_backward_begin</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4><a id=on_backward_end></a><code>on_backward_end</code>(<code>kwargs</code>:<code>Any</code>)<div style="text-align: right"><a href="https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L94">[source]</a></div></h4>
<p>Convert the gradients back to FP32 and divide them by the scale.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>MixedPrecision.on_backward_end</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4><a id=on_loss_begin></a><code>on_loss_begin</code></h4>
<p>(<code>last_output</code>:<code>Tensor</code>, <code>kwargs</code>:<code>Any</code>) -&gt; <code>Tensor</code><div style="text-align: right"><a href="https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L85">[source]</a></div></p>
<p>Converts half precision output to FP32 to avoid reduction overflow.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>MixedPrecision.on_loss_begin</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4><a id=on_step_end></a><code>on_step_end</code>(<code>kwargs</code>:<code>Any</code>) -&gt; <code>NoneType</code><div style="text-align: right"><a href="https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L100">[source]</a></div></h4>
<p>Update the params from master to model and zero grad</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>MixedPrecision.on_step_end</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4><a id=on_train_begin></a><code>on_train_begin</code>(<code>kwargs</code>:<code>Any</code>) -&gt; <code>NoneType</code><div style="text-align: right"><a href="https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L62">[source]</a></div></h4>
<p>Ensures everything is in half precision mode</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>MixedPrecision.on_train_begin</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4><a id=on_train_end></a><code>on_train_end</code>(<code>kwargs</code>:<code>Any</code>) -&gt; <code>NoneType</code><div style="text-align: right"><a href="https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L79">[source]</a></div></h4>
<p>Removes half precision transforms added at <code>on_train_begin</code></p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>MixedPrecision.on_train_end</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4><a id=model_g2master_g></a><code>model_g2master_g</code></h4>
<p>(<code>model_params</code>:<code>Sequence</code>[<code>Tensor</code>], <code>master_params</code>:<code>Sequence</code>[<code>Tensor</code>], <code>flat_master</code>:<code>bool</code>=<code>False</code>) -&gt; <code>NoneType</code><div style="text-align: right"><a href="https://github.com/fastai/fastai_pytorch/blob/master/fastai/callbacks/fp16.py#L29">[source]</a></div></p>
<p>Copies the model gradients to the master parameters for the optimizer step</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="http://docs.fast.ai/callbacks.fp16.html#model_g2master_g"><code>model_g2master_g</code></a></p>

</div>
</div>
</div>
</div>
 

